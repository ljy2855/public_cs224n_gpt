\documentclass{article}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{graphicx}

\title{Fine-tuning GPT-2 for Short Query Intent Classification}

\author{Jin Young Lee}

\begin{document}

\maketitle

\begin{abstract}
This report presents our implementation and experimental analysis of a GPT-2-based language model, fine-tuned across four distinct natural language tasks: sentiment analysis, paraphrase detection, sonnet generation, and intent classification for short user queries. Our study aims to bridge generative pretraining with downstream task-specific performance. We report baseline results, identify model limitations, and explore cloze-style formulation for classification and creative generation via autoregressive decoding.
\end{abstract}

\section{Introduction}
Recent advancements in natural language processing (NLP) demonstrate the effectiveness of transformer-based architectures. GPT-2, a decoder-only model, exhibits strong capabilities in both language understanding and generation. This project involves implementing core GPT-2 components, fine-tuning for classification and generation, and evaluating the model's performance across multiple tasks.

\section{Basic Implementation Tasks}
\subsection{GPT-2 Architecture Implementation}
Our model is based on a 12-layer GPT-2 with masked multi-head attention and feed-forward layers. We implement key components:
\begin{itemize}
\item \texttt{CausalSelfAttention} for masked self-attention.
\item \texttt{GPT2Layer} stacking attention and MLP blocks.
\item \texttt{GPT2Model} combining token and positional embeddings.
\end{itemize}

\subsection{Adam Optimizer Implementation}
We implement the Adam optimizer with bias correction and decoupled weight decay. The optimizer includes:
\begin{itemize}
\item Momentum terms for first and second moments
\item Bias correction for initial training steps
\item Weight decay decoupling for better regularization
\end{itemize}

\section{Basic Downstream Tasks}
We evaluate our GPT-2 implementation on three fundamental NLP tasks to validate the model's capabilities. Table \ref{tab:downstream_results} summarizes the performance across these tasks.

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Task} & \textbf{Dataset} & \textbf{Performance} \\
\midrule
Sentiment Analysis & SST (5-class) & 51.3\% accuracy \\
& CFIMDB (binary) & 97.6\% accuracy \\
\midrule
Paraphrase Detection & Quora Question Pairs & 75.2\% accuracy \\
\midrule
Sonnet Generation & Shakespeare Sonnets & CHRF: 0.68 \\
\bottomrule
\end{tabular}
\caption{Performance on Basic Downstream Tasks}
\label{tab:downstream_results}
\end{table}

The results demonstrate GPT-2's versatility across different NLP tasks. The model shows strong performance on binary sentiment classification (CFIMDB) and reasonable results on more complex tasks like paraphrase detection. For sonnet generation, the model successfully captures structural patterns while maintaining reasonable semantic coherence.

\section{Main Experiment: Short Query Intent Classification}
\subsection{Task Description}
As an extension, we examine GPT-2's ability to classify user intent from short, ambiguous queries. Users often provide minimal input (e.g., ``Weather?'', ``Pizza nearby''), posing a challenge for traditional NLP models due to limited context. We fine-tune our GPT-2 model using the MASSIVE dataset (SetFit/amazon\_massive\_intent\_en-US), evaluating its intent classification performance across queries of varying lengths.

\subsection{Dataset Analysis}
The MASSIVE dataset (SetFit/amazon\_massive\_intent\_en-US) is a comprehensive collection of user queries for intent classification. The dataset is split into:
\begin{itemize}
    \item Training set: 11,500 utterances
    \item Validation set: 2,030 utterances
    \item Test set: 2,970 utterances
\end{itemize}

The dataset covers 60 distinct intent labels across various domains. Table \ref{tab:intent_labels} shows the distribution of intent labels by category.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Domain} & \textbf{Intent Labels} \\
\midrule
Smart Home & iot\_hue\_light*, iot\_cleaning, iot\_coffee, iot\_wemo\_* \\
\midrule
Time Management & alarm\_*, calendar\_*, datetime\_* \\
\midrule
Media & play\_*, music\_*, audio\_volume\_* \\
\midrule
Information & weather\_query, news\_query, qa\_* \\
\midrule
Transport & transport\_*, recommendation\_* \\
\midrule
Other & takeaway\_*, cooking\_*, email\_*, lists\_*, general\_* \\
\bottomrule
\end{tabular}
\caption{Distribution of Intent Labels by Domain}
\label{tab:intent_labels}
\end{table}

Example utterances from the dataset demonstrate the diversity of user queries:
\begin{itemize}
    \item ``wake me up at nine am on friday'' (alarm\_set)
    \item ``stop'' (audio\_volume\_mute)
    \item ``make the lighting bit more warm here'' (iot\_hue\_lightchange)
    \item ``check when the show starts'' (calendar\_query)
\end{itemize}

The dataset's balanced distribution across these 60 intent classes and the variety of query lengths make it particularly suitable for evaluating models' ability to handle both short, ambiguous queries and longer, more explicit commands.

\subsection{Model Architecture and Training}
We implement a GPT-2-based classifier with the following specifications:
\begin{itemize}
    \item Base GPT-2 model with 768-dimensional hidden states
    \item Custom linear classification head
    \item Two fine-tuning modes: last-linear-layer and full-model
    \item Dropout rate of 0.3 for regularization
    \item AdamW optimizer with learning rate 1e-3
    \item Batch size of 8
    \item Cross-entropy loss function
    \item Early stopping based on development set accuracy
\end{itemize}

\subsection{Results and Analysis}
Our model achieves competitive performance on the MASSIVE dataset, with detailed analysis of both training dynamics and final performance metrics.

\subsubsection{Training Dynamics}
Figure \ref{fig:training_dynamics} shows the training and development loss curves over epochs for the full-model fine-tuning approach. The model demonstrates stable convergence with minimal overfitting, as evidenced by the parallel trends in training and development loss. The development loss plateaus after approximately 5 epochs, suggesting that the model effectively captures the underlying patterns in the intent classification task.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{full-model/loss_metrics.png}
\caption{Training and Development Loss over Epochs (Full-Model)}
\label{fig:training_dynamics}
\end{figure}

\subsubsection{Performance Metrics}
The model's performance is shown in Figure \ref{fig:accuracy_comparison}. The full-model fine-tuning approach achieves superior performance compared to last-linear-layer fine-tuning, demonstrating the benefits of allowing the entire model to adapt to the specific task. This improvement suggests that the model can capture more nuanced patterns in user queries when all parameters are updated.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{full-model/accuracy_metrics.png}
\caption{Accuracy Performance over Epochs (Full-Model)}
\label{fig:accuracy_comparison}
\end{figure}

\subsubsection{Comparison with Official Benchmark}
When compared to the official MASSIVE benchmark, our full-model fine-tuned GPT-2 achieves competitive results despite using significantly fewer computational resources. Table \ref{tab:benchmark_comparison} shows the comparison with other models from the original paper.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Type} & \textbf{Accuracy (\%)} \\
\midrule
\textbf{GPT-2 (Ours)} & Decoder-only (monolingual) & \textbf{85.3} \\
mT5 Enc Full & Encoder-decoder (multilingual) & 89.0 ± 1.1 \\
mT5 T2T Full & Encoder-decoder (multilingual) & 87.9 ± 1.2 \\
XLM-R Full & Encoder-only (multilingual) & 88.3 ± 1.2 \\
\bottomrule
\end{tabular}
\caption{Comparison with Official MASSIVE Benchmark}
\label{tab:benchmark_comparison}
\end{table}

The results demonstrate that our full-model fine-tuned GPT-2 achieves competitive performance while using significantly fewer computational resources. The model's ability to handle short queries effectively is particularly notable, with strong performance on both single-word commands and complex multi-turn interactions.

\section{Conclusion and Future Work}
Our project validates GPT-2's flexibility across structured (classification) and unstructured (generation) tasks. The additional short-query intent classification task underscores GPT-2's ability to handle minimal context using deep pretraining. Future directions include parameter-efficient fine-tuning (LoRA), incorporating rhyme constraints in generation, and second-order optimization (e.g., Shampoo) for faster convergence.

\appendix
\section{Appendix: Implementation Details}
Sanity checks passed for attention and optimizer modules. All experiments conducted on a single NVIDIA RTX 3080 GPU. Hyperparameters: learning rate $1\mathrm{e}{-4}$, batch size 16, epochs 5--10. For intent classification, we used standard accuracy and F1 metrics segmented by query length.

\end{document}