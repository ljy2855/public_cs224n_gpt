\documentclass{article}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{xcolor}

\title{Fine-tuning GPT-2 for Sentiment Analysis, Paraphrase Detection, Sonnet Generation, and Short Query Intent Classification}

\author{Anonymous Authors}

\begin{document}

\maketitle

\begin{abstract}
This report presents our implementation and experimental analysis of a GPT-2-based language model, fine-tuned across four distinct natural language tasks: sentiment analysis, paraphrase detection, sonnet generation, and intent classification for short user queries. Our study aims to bridge generative pretraining with downstream task-specific performance. We report baseline results, identify model limitations, and explore cloze-style formulation for classification and creative generation via autoregressive decoding.
\end{abstract}

\section{Introduction}
Recent advancements in natural language processing (NLP) demonstrate the effectiveness of transformer-based architectures. GPT-2, a decoder-only model, exhibits strong capabilities in both language understanding and generation. This project involves implementing core GPT-2 components, fine-tuning for classification and generation, and evaluating the model's performance across multiple tasks.

\section{Task Descriptions}
\subsection{Sentiment Analysis}
We fine-tune GPT-2 on two datasets: Stanford Sentiment Treebank (SST) and CFIMDB. For SST, a 5-class classification task, we utilize phrase-level sentiment annotations. For CFIMDB, we address binary sentiment classification on long movie reviews.

\subsection{Paraphrase Detection}
The Quora Question Pairs dataset is used to determine semantic similarity between sentence pairs. We reformulate the classification task as cloze-style generation, prompting the model with a paraphrase question and expecting a ``yes'' or ``no'' response.

\subsection{Sonnet Generation}
We train the model to generate Shakespearian sonnets. The task involves predicting the next token given prior lines, preserving rhyme and structure inherent to sonnets.

\subsection{Short Query Intent Classification}
As an extension, we examine GPT-2's ability to classify user intent from short, ambiguous queries. Users often provide minimal input (e.g., ``Weather?'', ``Pizza nearby''), posing a challenge for traditional NLP models due to limited context. We fine-tune our GPT-2 model using the MASSIVE dataset (SetFit/amazon\_massive\_intent\_en-US), evaluating its intent classification performance across queries of varying lengths. We focus on short queries to test the hypothesis that GPT-2's deep contextual embeddings enable it to disambiguate intent even under severe information sparsity.

\section{Model Implementation}
\subsection{GPT-2 Architecture}
Our model is based on a 12-layer GPT-2 with masked multi-head attention and feed-forward layers. We implement key components:
\begin{itemize}
\item \texttt{CausalSelfAttention} for masked self-attention.
\item \texttt{GPT2Layer} stacking attention and MLP blocks.
\item \texttt{GPT2Model} combining token and positional embeddings.
\end{itemize}

\subsection{Implementation Details}
The GPT-2 implementation consists of several key components:

\subsubsection{Embedding Layer}
The model uses two types of embeddings:
\begin{itemize}
\item Word embeddings: Maps input tokens to dense vectors
\item Position embeddings: Adds positional information to each token
\end{itemize}
These embeddings are combined and passed through a dropout layer.

\subsubsection{GPT-2 Layer}
Each GPT-2 layer contains:
\begin{itemize}
\item Multi-head self-attention with causal masking
\item Layer normalization before attention and feed-forward
\item Feed-forward network with GELU activation
\item Residual connections around each sub-layer
\end{itemize}

\subsubsection{Model Architecture}
The complete model:
\begin{itemize}
\item Processes input through embedding layers
\item Passes through 12 transformer layers
\item Applies final layer normalization
\item Returns both sequence outputs and last token representation
\end{itemize}

\subsection{Training Setup}
We use HuggingFace tokenizers and initialize with pre-trained weights. Optimization is performed using the Adam optimizer with bias correction and decoupled weight decay. Fine-tuning varies between last-layer tuning and full-model tuning.

\section{Sentiment Classification Results}
Using the classifier head on the last token's representation:
\begin{itemize}
\item SST last-layer dev accuracy: 46.2\%
\item SST full-model dev accuracy: 51.3\%
\item CFIMDB last-layer dev accuracy: 86.1\%
\item CFIMDB full-model dev accuracy: 97.6\%
\end{itemize}
The significant improvement in full-model fine-tuning for CFIMDB suggests that the deeper context captured in longer sequences benefits from broader parameter updates.

\section{Paraphrase Detection via Cloze-Style Prompting}
We construct cloze-formulated prompts like:
\texttt{Is ``sentence A'' a paraphrase of ``sentence B''?}

We decode the next token and compare it with token ids for ``yes'' or ``no''. Performance is measured via accuracy on Quora's dev/test sets.

\subsection{Observations}
Cloze formulation leverages GPT-2's autoregressive nature. Preliminary experiments show that decoder-only models can handle classification by token generation, although error rates arise from ambiguity or tokenization noise.

\section{Sonnet Generation}
GPT-2 is fine-tuned on 143 sonnets and evaluated on 12 held-out examples, conditioned on their first three lines. Evaluation metric is CHRF.

\subsection{Qualitative Analysis}
Generated outputs often preserve meter and rhyme patterns. However, semantic coherence and novelty vary. Sample outputs demonstrate syntactic fluency, but shallow semantic depth.

\section{Short Query Intent Classification Results}
We segment test samples from the MASSIVE dataset by length to compare performance on short versus long queries. The model shows promising F1-scores even on short utterances, though performance slightly declines as length decreases. Qualitative analysis reveals that ambiguity is more pronounced in imperative or single-word inputs.

\section{Conclusion and Future Work}
Our project validates GPT-2's flexibility across structured (classification) and unstructured (generation) tasks. The additional short-query intent classification task underscores GPT-2's ability to handle minimal context using deep pretraining. Future directions include parameter-efficient fine-tuning (LoRA), incorporating rhyme constraints in generation, and second-order optimization (e.g., Shampoo) for faster convergence.

\section*{Acknowledgments}
We thank the CS224N staff for providing the starter code and detailed documentation.

\appendix
\section{Appendix: Implementation Details}
Sanity checks passed for attention and optimizer modules. All experiments conducted on a single NVIDIA A100 GPU. Hyperparameters: learning rate $1\mathrm{e}{-4}$, batch size 16, epochs 5--10. For intent classification, we used standard accuracy and F1 metrics segmented by query length.

\end{document}